{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Innovative-Sol-Test.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "nTkUeDnEAQtz",
        "5bkEFoVHaq8V",
        "2Vd6I7xoL9J2",
        "qcZa0TZvPAir",
        "eicc49i4PSP7",
        "3MVT2hm_a51C",
        "R1av67mx7ElO",
        "ynwY1CnSa51C",
        "Srrnxva3a51D",
        "HXPFyBoMa51D",
        "fcSWHa3lCHeW",
        "Ycy0oS_b4WZs"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPMdZF1wn1aLub132Hi1/et",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhizarAziz/Test_Solution/blob/main/Innovative_Sol_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao6WFVKRFOXF"
      },
      "source": [
        "import numpy as np\n",
        "# from pathlib import Path\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTkUeDnEAQtz"
      },
      "source": [
        "# **Setup Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7OtTEi4ANv7"
      },
      "source": [
        "#Vision\n",
        "!gdown --id 1Gn8A2bfGK80JlYz9IU6GEWQP1NT8Jjgc\n",
        "!unzip -q dataset.zip # unzip zip file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB-4Y5G_D2Wq"
      },
      "source": [
        "# NLP\n",
        "!gdown --id 19YsuFeoRQI3CwEV5VvCBWhR5C9y_3xWW\n",
        "!gdown --id 1v-2WODjtFI6QL1XIiGqKr4u82466lRGu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znAZ365PEtM8"
      },
      "source": [
        "# **Vision**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bkEFoVHaq8V"
      },
      "source": [
        "\n",
        "\n",
        "> ## **Imports**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAviOdpZargE"
      },
      "source": [
        "from keras.layers import Input,Conv2D,BatchNormalization,ReLU,AveragePooling2D,GlobalAveragePooling2D,Dense,Dropout,multiply\n",
        "from keras.models import Model\n",
        "# from keras import regularizers\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.applications.mobilenet import MobileNet\n",
        "# from keras.callbacks import ReduceLROnPlateau"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vd6I7xoL9J2"
      },
      "source": [
        "\n",
        "\n",
        "> ## **Load & Preprocess Data**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNI43FCEL9J3"
      },
      "source": [
        "#paths\n",
        "train_dir = '/content/dataset/training_set/'\n",
        "test_dir = '/content/dataset/test_set'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOQRIhpVL9J3"
      },
      "source": [
        "#params\n",
        "input_shape = (224,224,3)\n",
        "dropout = 0.2\n",
        "batch_size = 32\n",
        "all_categories = [dirname for dirname in os.listdir(train_dir)]\n",
        "out_categories = len(all_categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VNPZiLBL9J3"
      },
      "source": [
        "#functions\n",
        "def get_dataset(base_dir):\n",
        "  onlyfiles = []\n",
        "  for dirpath, dirnames, filenames in os.walk(base_dir):\n",
        "    for filename in [f for f in filenames if f.endswith(\".jpg\")]:\n",
        "      onlyfiles.append([os.path.join(dirpath,filename),dirpath.split('/')[-1]])\n",
        "  random.shuffle(onlyfiles) # generalize better\n",
        "  return onlyfiles\n",
        "\n",
        "def data_generator(onlyfiles,img_shape,batch_size):\n",
        "  df_count = len(onlyfiles)\n",
        "  while True:\n",
        "    start = 0\n",
        "    while start+batch_size < df_count:\n",
        "      current_batch = onlyfiles[start:start+batch_size] # fetching a sub_df, which is our batch\n",
        "      #load imgs, normalize & create a list\n",
        "      img_List = []\n",
        "      train_labels = [] # list for 2_point_rep of ages\n",
        "      for item in current_batch: #iterate over batch to load & transform each img\n",
        "        img = cv2.imread(item[0])\n",
        "        ss = np.min(img.shape[0:2])\n",
        "        img = img[0:ss,0:ss] # crop_square\n",
        "        img = cv2.resize(img,img_shape[0:2])\n",
        "        img = img/255 # normalize\n",
        "        img_List.append(img)\n",
        "        \n",
        "        # labels encoding\n",
        "        label_id = all_categories.index(item[1])\n",
        "        label_enc = to_categorical(label_id,len(all_categories))\n",
        "        train_labels.append(label_enc)\n",
        "\n",
        "      img_np = np.array(img_List) \n",
        "      labels_np = np.array(train_labels)\n",
        "\n",
        "      yield img_np, labels_np # return batch\n",
        "      start += batch_size # update start point, for next batch\n",
        "\n",
        "def get_testset(onlyfiles,img_shape):\n",
        "  imgs = []\n",
        "  labels = []\n",
        "  for item in onlyfiles:\n",
        "    img = cv2.imread(item[0])\n",
        "    # ss = np.min(img.shape[0:2])\n",
        "    # img = img[0:ss,0:ss] # crop_square\n",
        "    img = cv2.resize(img,img_shape[0:2])\n",
        "    img = img/255 # normalize\n",
        "    imgs.append(img)\n",
        "    \n",
        "    # labels encoding\n",
        "    label_id = all_categories.index(item[1])\n",
        "    label_enc = to_categorical(label_id,len(all_categories))\n",
        "    labels.append(label_enc)\n",
        "  img_np = np.array(imgs) \n",
        "  labels_np = np.array(labels)  \n",
        "  return img_np,labels_np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxihsrZIL9J3"
      },
      "source": [
        "# train & val split\n",
        "dataset = get_dataset(train_dir)\n",
        "trainset, valset = train_test_split(dataset, train_size=0.8, test_size=0.2, random_state=5)\n",
        "train_gen = data_generator(trainset,input_shape ,batch_size)\n",
        "val_gen = data_generator(valset,input_shape ,batch_size)\n",
        "\n",
        "# testset\n",
        "testset = get_dataset(test_dir)\n",
        "test_imgs,test_labels = get_testset(testset,input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcZa0TZvPAir"
      },
      "source": [
        "\n",
        "\n",
        "> ## **Training**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd0cDdjBL9J3"
      },
      "source": [
        "# MODEL \n",
        "model = MobileNet(include_top=False,weights='imagenet',input_shape=input_shape)\n",
        "m = GlobalAveragePooling2D()(model.output)\n",
        "m = Dense(128,activation='relu')(m)\n",
        "m_out = Dense(out_categories,activation='softmax')(m)\n",
        "model = Model(inputs=[model.input],outputs=[m_out])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyDYn9sFL9J3"
      },
      "source": [
        "#COMPILE\n",
        "lr = 0.001\n",
        "adam = Adam(lr=lr)\n",
        "model.compile(\n",
        "    optimizer=adam,\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics='accuracy'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONdrZXtnL9J3"
      },
      "source": [
        "epochs = 15\n",
        "history = model.fit(train_gen,steps_per_epoch=len(trainset) / batch_size, epochs=epochs,validation_data=val_gen,  validation_steps=len(valset) / batch_size * 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NmDkVjVL9J3"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.grid(axis='both')\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.legend(['accuracy', 'val_accuracy'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eicc49i4PSP7"
      },
      "source": [
        "\n",
        "\n",
        "> ## **Evaluate**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtIUrXiNL9J3"
      },
      "source": [
        "# prediction\n",
        "p = model.evaluate(test_imgs,test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_SW7sjwZszS"
      },
      "source": [
        "print(f'Loss: {p[0]} -  Accuracy: {round(p[1]*100,3)}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrE4zne8aFvU"
      },
      "source": [
        "# **NLP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MVT2hm_a51C"
      },
      "source": [
        "\n",
        "\n",
        "> ## **Imports**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1jOdZrca51C"
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Embedding,Dense,Dropout,LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth',100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1av67mx7ElO"
      },
      "source": [
        "> ## **Load & Preprocess Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSHzNy_jKney"
      },
      "source": [
        "train_df = pd.read_csv('/content/twitter_train.csv',encoding = \"ISO-8859-1\")\n",
        "train_df = train_df[['Sentiment','OriginalTweet']]\n",
        "test_df = pd.read_csv('/content/twitter_test.csv',encoding = \"ISO-8859-1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br0RbRJsosIG"
      },
      "source": [
        "#CREATING LABELS\n",
        "all_categories = train_df['Sentiment'].unique()\n",
        "out_categories = len(all_categories)\n",
        "labels = []\n",
        "for i in train_df['Sentiment']:\n",
        "  label_id = np.where(all_categories == i)\n",
        "  label_enc = to_categorical(label_id,out_categories)\n",
        "  labels.append(label_enc[0])\n",
        "labels = np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHXrsdQTnz_v"
      },
      "source": [
        "#VALIDATIO SPLIT\n",
        "x_train,x_test,y_train,y_test = train_test_split(train_df['OriginalTweet'],labels,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8bawqixmbEM"
      },
      "source": [
        "# initialize and fit tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GljdpvslnDqv"
      },
      "source": [
        "#use tokenizer to trnsfrm txt msgz in train and test sets\n",
        "x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
        "x_test_seq = tokenizer.texts_to_sequences(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTsgo_aHrBMC"
      },
      "source": [
        "# int representation of first tweet\n",
        "len(x_train_seq[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbUTHC9IrJPM"
      },
      "source": [
        "# add padding to equalize size of each tweet\n",
        "x_train_seq_padded = pad_sequences(x_train_seq,80)\n",
        "x_test_seq_padded = pad_sequences(x_test_seq,80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dAw1yu27JEy"
      },
      "source": [
        "> ## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8gkYwlRlv02"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(len(tokenizer.index_word)+1,32)) # Creating vectors (vectorization inside model) of length 32\n",
        "model.add(LSTM(32,dropout=0,recurrent_dropout=0)) # type of rnn\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dense(5,activation='sigmoid'))\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U2OEupz7RDj"
      },
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47MyyZsYs2au"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics = ['accuracy',precision_m,recall_m]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxLc5jyMlMIe"
      },
      "source": [
        "history = model.fit(x_train_seq_padded,y_train,batch_size=32,epochs=10,\n",
        "          validation_data=(x_test_seq_padded,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeQRvDdAKn-m"
      },
      "source": [
        "plt.plot(history.history['precision_m'])\n",
        "plt.grid(axis='both')\n",
        "plt.plot(history.history['val_precision_m'])\n",
        "plt.legend(['precision_m', 'val_precision_m'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvvEk12UKn7L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynwY1CnSa51C"
      },
      "source": [
        "\n",
        "\n",
        "> ## **Load & Preprocess Data**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srrnxva3a51D"
      },
      "source": [
        "\n",
        "\n",
        "> ## **Training**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXPFyBoMa51D"
      },
      "source": [
        "\n",
        "\n",
        "> ## **Evaluate**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56h0qSaya51D"
      },
      "source": [
        "# prediction\n",
        "p = model.evaluate(test_imgs,test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWIyP8YEa51D"
      },
      "source": [
        "print(f'Loss: {p[0]} -  Accuracy: {round(p[1]*100,3)}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdOwI4Pha4FB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd5-FgzxaQ--"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcSWHa3lCHeW"
      },
      "source": [
        "# **LOAD AND CLEAN DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBZLL-lY5z3J"
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth',100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k__QZcedbF10"
      },
      "source": [
        "train_df = pd.read_csv('/content/twitter_train.csv',encoding = \"ISO-8859-1\")\n",
        "train_df = train_df[['Sentiment','OriginalTweet']]\n",
        "test_df = pd.read_csv('/content/twitter_test.csv',encoding = \"ISO-8859-1\")\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "ps = nltk.PorterStemmer()\n",
        "wn = nltk.WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdM8o0AITTKR"
      },
      "source": [
        "trainset, testset = train_test_split(train_df, train_size=0.7, test_size=0.3, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdIpGTM4G0OD"
      },
      "source": [
        "def clean_text(text):\n",
        "  # remove punctuation\n",
        "  text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
        "  # tokenize\n",
        "  tokens = re.findall('\\w+',text_nopunct) #re.split('\\W+',text_nopunct)\n",
        "  # remove stop words\n",
        "  no_stopwords = [word for word in tokens if word not in stopwords]\n",
        "  #LEMMATIZINNG (slow but more accurate)\n",
        "  lemmatized = [wn.lemmatize(word) for word in no_stopwords]\n",
        "  #stemmatized = [ps.stem(word) for word in no_stopwords]\n",
        "  return lemmatized\n",
        "trainset['CleanedTweet'] = trainset['OriginalTweet'].apply(lambda x: clean_text(x.lower()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RECmj7nTidZm"
      },
      "source": [
        "# vectorizing data\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vect = TfidfVectorizer(analyzer = clean_text) # clean_text() defined above\n",
        "X_tfidf = tfidf_vect.fit_transform(trainset['OriginalTweet'])\n",
        "print('shape of df will be ',X_tfidf.shape) \n",
        "# print('columns names are',tfidf_vect.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0488TZZ0T_bA"
      },
      "source": [
        "x_train = pd.DataFrame(X_tfidf.toarray())\n",
        "y_train = trainset['Sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIKmFiZ74RFG"
      },
      "source": [
        "x_train_np = x_train.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycy0oS_b4WZs"
      },
      "source": [
        "# **MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDkTXzn-3JgZ"
      },
      "source": [
        "from keras.layers import Embedding,Flatten, Dense\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQLAbjuI29aL"
      },
      "source": [
        "vocab_size = 5000\n",
        "embedding_dim = 50\n",
        "seq_len = 500\n",
        "#MODEL\n",
        "dense_model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=seq_len),\n",
        "    Flatten(),\n",
        "    Dense(1, activation='relu')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7G030XU3DkQ"
      },
      "source": [
        "dense_model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "dense_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL7kYDP_4YsE"
      },
      "source": [
        "dense_model.fit(x_train_np, y_train, validation_data=(x_train_np, y_train), epochs=5, batch_size=64)\n",
        "# dense_model.save_weights(\"model/dense.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqL7jzKpUB0L"
      },
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ituMKAySYL9"
      },
      "source": [
        "# x_train, x_test, y_train, y_test = train_test_split(pd.DataFrame(X_tfidf.toarray()),train_df['Sentiment'],test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk5MQSOuKkky"
      },
      "source": [
        "# X_features = pd.DataFrame(X_tfidf.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsvIdBBOPhSE"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zgi1QZ1MI6P"
      },
      "source": [
        "# **playground**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODfuZI4Y7VpW"
      },
      "source": [
        "import gensim\n",
        "import gensim.downloader as api\n",
        "wiki_embeddings = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFHqvrrW7qZv"
      },
      "source": [
        "wiki_embeddings['king']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-XSvElU72-u"
      },
      "source": [
        "wiki_embeddings.most_similar('king')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiQJmtsq8N3_"
      },
      "source": [
        "trainset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EQ7ynxZ78Kd"
      },
      "source": [
        "train_df['gensim_cleaned'] = train_df['OriginalTweet'].apply(lambda x: gensim.utils.simple_preprocess(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncA9L2eh-Bpp"
      },
      "source": [
        "train_test_split(trainset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUrZlhNu98tc"
      },
      "source": [
        "x_train, x_test, y_train,ytest = train_test_split(train_df['gensim_cleaned'],\n",
        "                train_df['Sentiment'],test_size =0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfmV6BZu98xB"
      },
      "source": [
        "#training our own wordtovec\n",
        "w2v_model = gensim.models.Word2Vec(x_train, #convert this to vectors\n",
        "                                   size=100, # size of vector we want\n",
        "                                   window=5, # window of context words to consider at once, like in tut\n",
        "                                   min_count=2) # number of times word needs to atleast appear in our corpus of data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzSy7WuS_4zj"
      },
      "source": [
        "w2v_model.wv['king']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uWiXayLAB8n"
      },
      "source": [
        "w2v_model.wv.most_similar('happy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rR8G5yUAhFC"
      },
      "source": [
        "w2v_model.wv.index2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R_h84d8AoQB"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HStdq_Li981r"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY7C1DWq9841"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVlST7hh988r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C3IZie399As"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT7eeQI799Dk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxMmDonVrzJq"
      },
      "source": [
        "# count length of tweet (probably not useful)... anyways!\n",
        "train_df['AlphabetCount'] = train_df['OriginalTweet'].apply(lambda x: len(x) - x.count(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKzvyV6L4GIX"
      },
      "source": [
        "# feature engineering\n",
        "def count_punct(text):\n",
        "  total_puncts = sum([1 for char in text if char in string.punctuation])\n",
        "  return round((total_puncts * 100) / len(text),3)\n",
        "train_df['PunctutationCount'] = train_df['OriginalTweet'].apply(lambda x: count_punct(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5JgMGxV5Wsx"
      },
      "source": [
        "bins = np.linspace(0,800,40)\n",
        "plt.hist([1,2,3,4],)\n",
        "plt.hist(train_df[train_df['Sentiment'] == 'Neutral']['AlphabetCount'],bins,alpha= 0.5,  label='Neutral')\n",
        "plt.hist(train_df[train_df['Sentiment'] == 'Positive']['AlphabetCount'],bins,alpha= 0.5,  label='Positive')\n",
        "plt.hist(train_df[train_df['Sentiment'] == 'Extremely Negative']['AlphabetCount'],bins,alpha= 0.5,  label='Extremely Negative')\n",
        "plt.hist(train_df[train_df['Sentiment'] == 'Negative']['AlphabetCount'],bins,alpha= 0.5,  label='Negative')\n",
        "plt.hist(train_df[train_df['Sentiment'] == 'Extremely Positive']['AlphabetCount'],bins,alpha= 0.5,  label='Extremely Positive')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kHDhKjRm_Uw"
      },
      "source": [
        "train_df.head(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBMWzwlG5DQk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0foFkwELsM7d"
      },
      "source": [
        "# string.punctuation in 'i am khizer.'\n",
        "sum([1 for char in 'i am.  5 &#$%^ khizer.' if char in string.punctuation])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1si7cNj4J6b"
      },
      "source": [
        "#tokenization (findall(), split())\n",
        "s = 'helo is isss  ,  :: \"\"\"\"  my name'\n",
        "re.findall('\\w+',s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr6EwGU7sMq4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erPC0d-U6PG2"
      },
      "source": [
        "s = 'I follow PEP8, pep9 & pep 8 Guidlines'\n",
        "re.findall('[A-Za-z/t]+[0-9]+',s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8saWD5xz7QmA"
      },
      "source": [
        "s = 'I follow PEP8 Guidlines & i was born in 1992 when i was the leader in 123'\n",
        "re.findall('[0-9]+',s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPIRTZY3-DbG"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt8p6h7Fmssd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqwFOZvjMBi0"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MZc8Cd0Lebl"
      },
      "source": [
        "#LEMMATIZINNG (slow but more accurate)\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "def lemmatizing(tokenized_text):\n",
        "  text = [wn.lemmatize(word) for word in tokenized_text]\n",
        "  return text\n",
        "\n",
        "train_df['lemmatized_tweets'] = train_df['cleaned_text'].apply(lambda x: lemmatizing(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExNWv4EiH9Z9"
      },
      "source": [
        "#STEMMING (more faster than lemmizer)\n",
        "ps = nltk.PorterStemmer()\n",
        "def stemming(tokenized_text):\n",
        "  text = [ps.stem(word) for word in tokenized_text]\n",
        "  return text\n",
        "\n",
        "train_df['stemmed_tweets'] = train_df['cleaned_text'].apply(lambda x: stemming(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SmDrxlbInfG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl7YeSpeJ_SU"
      },
      "source": [
        "print(ps.stem('meaning'))\n",
        "print(ps.stem('meanness'))\n",
        "print(wn.lemmatize('meaning'))\n",
        "print(wn.lemmatize('meanness'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN23syntK-IN"
      },
      "source": [
        "print(ps.stem('goose'))\n",
        "print(ps.stem('geese'))\n",
        "print(wn.lemmatize('goose'))\n",
        "print(wn.lemmatize('geese'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoLsB157HdIq"
      },
      "source": [
        "train_df.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYPbwtVB-yAA"
      },
      "source": [
        "# REMOVE PUNCTUATIONS\n",
        "\n",
        "def remove_punct(text):\n",
        "  text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
        "  return text_nopunct\n",
        "\n",
        "train_df['OriginalTweet_clean'] = train_df['OriginalTweet'].apply(lambda x: remove_punct(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kA6N43mAyKI"
      },
      "source": [
        "# TOKENIZATION\n",
        "def tokenize(text):\n",
        "  tokens = re.split('\\W+',text)\n",
        "  return tokens\n",
        "\n",
        "train_df['OriginalTweet_tokenized'] = train_df['OriginalTweet_clean'].apply(lambda x: tokenize(x.lower()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtmkD3iMCh-S"
      },
      "source": [
        "# REMOVE STOP WORDS\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "def remove_stopwords(tokenized_list):\n",
        "  text = [word for word in tokenized_list if word not in stopwords]\n",
        "  return text\n",
        "\n",
        "train_df['not_stop_words'] = train_df['OriginalTweet_tokenized'].apply(lambda x: remove_stopwords(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgRaRGz6Emrc"
      },
      "source": [
        "#STEMMING\n",
        "ps = nltk.PorterStemmer()\n",
        "def stemming(tokenized_text):\n",
        "  text = [word for word in tokenized_text]\n",
        "  return text\n",
        "\n",
        "train_df['stemmed_tweets'] = train_df['OriginalTweet_tokenized'].apply(lambda x: stemming(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWmKskW8p00W"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxzIDDmvDTXK"
      },
      "source": [
        "train_df.iloc[1:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhmzF68NEuYm"
      },
      "source": [
        "print(ps.stem(\"grow\"))\n",
        "print(ps.stem(\"grows\"))\n",
        "print(ps.stem(\"growing\"))\n",
        "print(ps.stem(\"grown\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK10ZuPtFYqx"
      },
      "source": [
        "print(ps.stem(\"mean\"))\n",
        "print(ps.stem(\"meanness\"))\n",
        "print(ps.stem(\"meaning\"))\n",
        "print(ps.stem(\"meaningless\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMQVlr7aFc9L"
      },
      "source": [
        "print(ps.stem(\"run\"))\n",
        "print(ps.stem(\"running\"))\n",
        "print(ps.stem(\"runner\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDuwb_Rdqfz6"
      },
      "source": [
        "train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VfPCualibHN"
      },
      "source": [
        "stopwords.words()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmeZoOhpjduB"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI3HJPBFbGuV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPol60ikMK16"
      },
      "source": [
        "ind = 12\n",
        "batch = next(iter(train_gen))\n",
        "print(batch[1][ind])\n",
        "img = batch[0][ind]\n",
        "plt.imshow(batch[0][ind])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}